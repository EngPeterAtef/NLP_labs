{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5998a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type Your full names\n",
    "Student_1 = \"\"\n",
    "Student_2 = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0858b2ed",
   "metadata": {},
   "source": [
    "# Word Embeddings Assignment\n",
    "We learnt about how to train word embeddings and how these embeddings can represent the meaning of words. Of course you don't need to train your word embeddings model every time you need to use them. In this assignment, we will use pre-trained word embeddings to see some real world usages of them. There are two types of questions in this assignment:\n",
    "1. Coding questions: where you will write normal code\n",
    "2. Essay questions: where you will need to write a sentence to answer the provided questions so don't forget to write the answers for this type\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "# Predict the country from its capital\n",
    "In this assignment, our main goal is to use word embeddings to predict the country given its capital just by applying some vector operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3162cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import get_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f04faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = pd.read_csv('capitals.txt', delimiter=' ')\n",
    "data.columns = ['city1', 'country1', 'city2', 'country2']\n",
    "\n",
    "# print first five elements in the DataFrame\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53389420",
   "metadata": {},
   "source": [
    "## Loading the word embeddings\n",
    "We will work with google news word embedding dataset but the original data is about 3.4 GigaBytes. To remove this problem, You will only need to work with the provided file `word_embeddings_subset.p` that contains a very small subset of the original embeddings that we will need to use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f132f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = pickle.load(open(\"word_embeddings_subset.p\", \"rb\"))\n",
    "print(len(word_embeddings))\n",
    "print(word_embeddings['Spain'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be33643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## TODO: Answer the following questions #####################################\n",
    "# Q1: What does the 243 printed above represent?\n",
    "A1 = \"\"\"\n",
    "Write your answer here\n",
    "\"\"\"\n",
    "\n",
    "# Q2: What does the (300,) printed above represent?\n",
    "A2 = \"\"\"\n",
    "Write Your answer here\n",
    "\"\"\"\n",
    "#############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d955c3",
   "metadata": {},
   "source": [
    "## Compute the cosine similarity\n",
    "The cosine similarity between two vectors:\n",
    "\n",
    "$$\\cos (\\theta)=\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d22a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "    '''\n",
    "    Input:\n",
    "        A: a numpy array which corresponds to a word vector\n",
    "        B: A numpy array which corresponds to a word vector\n",
    "    Output:\n",
    "        cos: numerical number representing the cosine similarity between A and B.\n",
    "    '''\n",
    "    cos = None\n",
    "    ############################# TODO: Compute the cosine similarity ####################################\n",
    "    \n",
    "    ######################################################################################################\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f97520",
   "metadata": {},
   "outputs": [],
   "source": [
    "king = word_embeddings['king']\n",
    "queen = word_embeddings['queen']\n",
    "print(cosine_similarity(king, queen))\n",
    "assert cosine_similarity(king, queen) - 0.6510956 < 1e-6, \"Cosine Similarity Error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12ec5a9",
   "metadata": {},
   "source": [
    "## Finding the country of each capital\n",
    "\n",
    "Now, you  will use the previous function to compute similarities between vectors,\n",
    "and use these to find the capital cities of countries. You will write a function that\n",
    "takes in three words, and the embeddings dictionary. Your task is to find the country of \n",
    "capital cities. For example, given the following words: \n",
    "\n",
    "- 1: Athens 2: Greece 3: Baghdad,\n",
    "\n",
    "your task is to predict the country 4: Iraq.\n",
    "\n",
    "**Instructions**: \n",
    "\n",
    "1. To predict the country of the capital you might want to look at the *King - Man + Woman = Queen* example, and implement that scheme into a mathematical function, using the word embeddings and a similarity function.\n",
    "\n",
    "2. Iterate over the embeddings dictionary and compute the cosine similarity score between your vector and the current word embedding.\n",
    "\n",
    "3. You should add a check to make sure that the word you return is not any of the words that you fed into your function. Return the one with the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5c09ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country(city1, country1, city2, embeddings):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        city1: a string (the capital city of country1)\n",
    "        country1: a string (the country of capital1)\n",
    "        city2: a string (the capital city of country2)\n",
    "        embeddings: a dictionary where the keys are words and values are their embeddings\n",
    "    Output:\n",
    "        country: a tuple containing the word and the similarity score\n",
    "    \"\"\"\n",
    "    ################################## TODO: Implement the following lines ######################################\n",
    "\n",
    "    # store the city1, country 1, and city 2 in a set called group\n",
    "    group = set((None, None, None))\n",
    "\n",
    "    # get embeddings of city 1\n",
    "    city1_emb = None\n",
    "\n",
    "    # get embedding of country 1\n",
    "    country1_emb = None\n",
    "\n",
    "    # get embedding of city 2\n",
    "    city2_emb = None\n",
    "\n",
    "    # get embedding of country 2 (it's a combination of the embeddings of country 1, city 1 and city 2)\n",
    "    # Remember: King - Man + Woman = Queen\n",
    "    vec = None\n",
    "\n",
    "    # Initialize the similarity to -1 (it will be replaced by a similarities that are closer to +1)\n",
    "    similarity = -1\n",
    "\n",
    "    # initialize country to an empty string\n",
    "    country = ''\n",
    "\n",
    "    # loop through all words in the embeddings dictionary\n",
    "    for word in embeddings.keys():\n",
    "\n",
    "        # first check that the word is not already in the 'group'\n",
    "        if word not in group:\n",
    "\n",
    "            # get the word embedding\n",
    "            word_emb = None\n",
    "\n",
    "            # calculate cosine similarity between embedding of country 2 and the word in the embeddings dictionary\n",
    "            cur_similarity = None\n",
    "\n",
    "            # if the cosine similarity is more similar than the previously best similarity...\n",
    "            if cur_similarity > similarity:\n",
    "\n",
    "                # update the similarity to the new, better similarity\n",
    "                similarity = None\n",
    "\n",
    "                # store the country as a tuple, which contains the word and the similarity\n",
    "                country = (None, None)\n",
    "\n",
    "    #########################################################################################################\n",
    "\n",
    "    return country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5665d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing your function\n",
    "country_test, similarity_test = get_country('Athens', 'Greece', 'Cairo', word_embeddings)\n",
    "print(country_test, similarity_test, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d291c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### TODO: Answer The following question #############################\n",
    "# Q3: Is the country predicted above is what you expected?\n",
    "A3 = \"\"\"\n",
    "Type Yes or No\n",
    "\"\"\"\n",
    "\n",
    "# Q4: Does the similarity printed above make sense? Why?\n",
    "A4 = \"\"\"\n",
    "Type Yes or No. and the reason for your answer.\n",
    "\"\"\"\n",
    "#####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118dd78e",
   "metadata": {},
   "source": [
    "## Model Accuracy\n",
    "\n",
    "Now you will test your new function on the dataset and check the accuracy of the model:\n",
    "\n",
    "$$\\text{Accuracy}=\\frac{\\text{Correct # of predictions}}{\\text{Total # of predictions}}$$\n",
    "\n",
    "**Instructions**: Write a program that can compute the accuracy on the dataset provided for you. You have to iterate over every row to get the corresponding words and feed them into you `get_country` function above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0336f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(word_embeddings, data):\n",
    "    '''\n",
    "    Input:\n",
    "        word_embeddings: a dictionary where the key is a word and the value is its embedding\n",
    "        data: a pandas dataframe containing all the country and capital city pairs\n",
    "    \n",
    "    Output:\n",
    "        accuracy: the accuracy of the model\n",
    "    '''\n",
    "\n",
    "    ################################ TODO: Implement this function ####################################\n",
    "    # initialize num correct to zero\n",
    "    num_correct = 0\n",
    "\n",
    "    # loop through the rows of the dataframe\n",
    "    for i, row in data.iterrows():\n",
    "\n",
    "        # get city1\n",
    "        city1 = None\n",
    "\n",
    "        # get country1\n",
    "        country1 = None\n",
    "\n",
    "        # get city2\n",
    "        city2 =  None\n",
    "\n",
    "        # get country2\n",
    "        country2 = None\n",
    "\n",
    "        # use get_country to find the predicted country2\n",
    "        predicted_country2, _ = None\n",
    "\n",
    "        # if the predicted country2 is the same as the actual country2...\n",
    "        if predicted_country2 == country2:\n",
    "            # increment the number of correct by 1\n",
    "            num_correct += None\n",
    "\n",
    "    # get the number of rows in the data dataframe (length of dataframe)\n",
    "    m = len(data)\n",
    "\n",
    "    # calculate the accuracy by dividing the number correct by m\n",
    "    accuracy = None\n",
    "\n",
    "    ##################################################################################################\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3fd7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell may take about 30 seconds to run so don't worry\n",
    "accuracy = get_accuracy(word_embeddings, data)\n",
    "print(f\"Accuracy is {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86043ef",
   "metadata": {},
   "source": [
    "# Plotting the vectors using PCA\n",
    "\n",
    "Now you will explore the distance between word vectors after reducing their dimension. You should have known above the dimensionality of the word embeddings. Of course we can't visualize such a high dimension of vectors so we need to use some dimensionality reduction technique to reduce the dimensions to 2 so we can visualize them. Since you studied before PCA, we will use it for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8583b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pca(X, n_components=2):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X: of dimension (m,n) where each row corresponds to a word vector\n",
    "        n_components: Number of components you want to keep.\n",
    "    Output:\n",
    "        X_reduced: data transformed in 2 dims/columns + regenerated original data\n",
    "    \"\"\"\n",
    "    X_reduced = None\n",
    "    ####################### TODO: Use sklearn.decomposition.PCA to implement this function #####################\n",
    "    \n",
    "    ###########################################################################################################\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94395a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['oil', 'gas', 'happy', 'sad', 'city', 'town',\n",
    "         'village', 'country', 'continent', 'petroleum', 'joyful']\n",
    "\n",
    "# given a list of words and the embeddings, it returns a matrix with all the embeddings\n",
    "X = get_vectors(word_embeddings, words)\n",
    "\n",
    "print('You have 11 words each of 300 dimensions thus X.shape is:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96439ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = compute_pca(X, 2)\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0] - 0.05, result[i, 1] + 0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f05e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### TODO: Answer the following Question ####################################\n",
    "# Q5: Does the plot above make sense?\n",
    "A5 = \"\"\"\n",
    "Type Yes or No\n",
    "\"\"\"\n",
    "\n",
    "# Q6: Illustrate the plot above\n",
    "A6 = \"\"\"\n",
    "Type your answer here\n",
    "\"\"\"\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d7959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = f\"\"\"\n",
    "Student 1: {Student_1}\n",
    "Student 2: {Student_2}\n",
    "\n",
    "Q1:\n",
    "{A1}\n",
    "\n",
    "Q2:\n",
    "{A2}\n",
    "\n",
    "Predicted Country:\n",
    "{country_test}\n",
    "\n",
    "Predicted Similarity:\n",
    "{similarity_test}\n",
    "\n",
    "Q3:\n",
    "{A3}\n",
    "\n",
    "Q4:\n",
    "{A4}\n",
    "\n",
    "Computed Accuracy:\n",
    "{accuracy}\n",
    "\n",
    "Q5:\n",
    "{A5}\n",
    "\n",
    "Q6:\n",
    "{A6}\n",
    "\"\"\"\n",
    "\n",
    "with open('answers.txt', 'w') as f:\n",
    "    f.write(answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
