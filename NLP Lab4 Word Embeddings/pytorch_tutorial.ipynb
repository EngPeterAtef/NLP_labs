{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# tensor filled with uninitialized data\n",
    "em = torch.empty(2)\n",
    "print(em) #by defulat initialized by zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "em = torch.empty(1,2)\n",
    "print(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.5000, 0.1000])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Initializing Tensor Directly from data\n",
    "x = torch.tensor([2.5, .1])\n",
    "print(x)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8037, 0.0587, 0.6570],\n",
      "        [0.9775, 0.8388, 0.7215],\n",
      "        [0.4194, 0.6211, 0.6200],\n",
      "        [0.0828, 0.3836, 0.4348],\n",
      "        [0.0327, 0.8909, 0.3442]])\n",
      "tensor([0.8037, 0.9775, 0.4194, 0.0828, 0.0327])\n",
      "torch.Size([5, 3])\n",
      "<class 'torch.Size'>\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "print(x)\n",
    "print(x[:, 0])\n",
    "print(x.size())\n",
    "print(type(x.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 58,  64],\n",
       "        [139, 154]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3],\n",
    "                [4, 5, 6]]) #2 * 3\n",
    "b = torch.tensor([[7, 8], [9, 10], [11, 12]]) #3*2\n",
    "c = torch.matmul(a, b) #2*3 3*2 -> 2*2\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statistical operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min row: torch.return_types.min(\n",
      "values=tensor([1., 3.]),\n",
      "indices=tensor([0, 0]))\n",
      "min col: torch.return_types.min(\n",
      "values=tensor([1., 2.]),\n",
      "indices=tensor([0, 0]))\n",
      "tensor_max:\n",
      "tensor(4.)\n",
      "tensor_mean:\n",
      "tensor(2.5000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tensor = torch.Tensor([[1, 2],\n",
    "                       [3, 4]])\n",
    "# row = 1 and col = 0\n",
    "# get minmum value in each row\n",
    "tensor_min_row = torch.min(tensor, dim=1)\n",
    "# get minmum value in each col\n",
    "tensor_min_col = torch.min(tensor, dim=0)\n",
    "# get max value in tensor\n",
    "tensor_max = torch.max(tensor)\n",
    "# calculate mean value\n",
    "tensor_mean = torch.mean(tensor)\n",
    "...  # Every single mathematical function you could imagine.\n",
    "print(\"min row:\", tensor_min_row)\n",
    "print(\"min col:\", tensor_min_col)\n",
    "\n",
    "print(\"tensor_max:\",tensor_max, \"tensor_mean:\",tensor_mean,sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_mean_row tensor([3., 4.])\n",
      "tensor_mean_col tensor([1.5000, 3.5000, 5.5000])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.Tensor([[1, 2],\n",
    "                       [3, 4],\n",
    "                       [5, 6]])  # shape = (3, 2)\n",
    "\n",
    "tensor_mean_row = torch.mean(tensor, dim=0)  # shape = (2,) Averaging over 1st dimension (along columns)\n",
    "tensor_mean_col = torch.mean(tensor, dim=1)  # shape = (3,) Averaging over 2nd dimension (along rows)\n",
    "\n",
    "print(\"tensor_mean_row\", tensor_mean_row) \n",
    "print(\"tensor_mean_col\", tensor_mean_col) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!![image.png](https://i.stack.imgur.com/ORqaP.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1627, 0.1329, 0.0312, 0.3476],\n",
      "        [0.5599, 0.7680, 0.3398, 0.5478],\n",
      "        [0.9847, 0.0399, 0.6875, 0.2004],\n",
      "        [0.6235, 0.6870, 0.6036, 0.2973]])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4,4)\n",
    "print(x)\n",
    "# view function is a view of the tensor, it does not create a new tensor\n",
    "# but modifies the original tensor and changes the shape\n",
    "y = x.view(-1,4 )\n",
    "\n",
    "z = x.view(1,16 )\n",
    "print(y.size())\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "type of b <class 'numpy.ndarray'>\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "type of c <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b= a.numpy()\n",
    "print(b)\n",
    "print(f\"type of b {type(b)}\")\n",
    "c = torch.from_numpy(b)\n",
    "print(c)\n",
    "print(f\"type of c {type(c)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    x = torch.ones(5, device=device)\n",
    "    y = torch.ones(5)\n",
    "    # By default, tensors are created on the CPU. We need to explicitly move tensors \n",
    "    # to the GPU using .to method (after checking for GPU availability)\n",
    "    y = y.to(device)\n",
    "    z = x*y\n",
    "    # z is in GPU 34an darbt 2 vectors fel gpu\n",
    "    z=z.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The general framework for training your model is as follows\n",
    "for epoch in range(num_epochs):\n",
    "\tfor i in range(num_batches):\n",
    "\t\tForward pass\n",
    "\t\tLoss calculation\n",
    "\t\tBackward pass\n",
    "\t\tWeights update\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Combines arrays in a vertically stacked sequence (used for data manipulation)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vstack\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Reads a CSV file into a DataFrame (used for loading datasets)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_csv\n",
      "File \u001b[1;32mc:\\Users\\Peter\\anaconda3\\Lib\\site-packages\\numpy\\__init__.py:141\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Allow distributors to run custom init code\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _distributor_init\n\u001b[1;32m--> 141\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n",
      "File \u001b[1;32mc:\\Users\\Peter\\anaconda3\\Lib\\site-packages\\numpy\\core\\__init__.py:73\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numerictypes \u001b[38;5;28;01mas\u001b[39;00m nt\n\u001b[0;32m     72\u001b[0m multiarray\u001b[38;5;241m.\u001b[39mset_typeDict(nt\u001b[38;5;241m.\u001b[39msctypeDict)\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numeric\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumeric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fromnumeric\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1069\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:731\u001b[0m, in \u001b[0;36m_compile_bytecode\u001b[1;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:244\u001b[0m, in \u001b[0;36m_verbose_message\u001b[1;34m(message, verbosity, *args)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Combines arrays in a vertically stacked sequence (used for data manipulation)\n",
    "from numpy import vstack\n",
    "\n",
    "# Reads a CSV file into a DataFrame (used for loading datasets)\n",
    "from pandas import read_csv\n",
    "\n",
    "# Encodes categorical labels into numerical format (used for label preprocessing)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Calculates the accuracy of a classification model (used for model evaluation)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Defines a custom dataset class for PyTorch (used for handling data)\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Creates a DataLoader for efficient batch processing in PyTorch (used for data loading)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Splits a dataset into training and validation sets (used for data splitting)\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Represents a multi-dimensional matrix in PyTorch (used for tensor manipulation)\n",
    "from torch import Tensor\n",
    "\n",
    "# Implements a linear layer in a neural network (used for defining neural network architecture)\n",
    "from torch.nn import Linear\n",
    "\n",
    "# Applies rectified linear unit (ReLU) activation function (used for introducing non-linearity)\n",
    "from torch.nn import ReLU\n",
    "\n",
    "# Applies sigmoid activation function (used for binary classification output)\n",
    "from torch.nn import Sigmoid\n",
    "\n",
    "# Base class for all neural network modules in PyTorch (used for creating custom models)\n",
    "from torch.nn import Module\n",
    "\n",
    "# Stochastic Gradient Descent optimizer (used for model optimization during training)\n",
    "from torch.optim import SGD\n",
    "\n",
    "# Binary Cross Entropy Loss function (used for binary classification problems)\n",
    "from torch.nn import BCELoss\n",
    "\n",
    "# Initializes weights using Kaiming uniform initialization (used for weight initialization)\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "\n",
    "# Initializes weights using Xavier (Glorot) uniform initialization (used for weight initialization)\n",
    "from torch.nn.init import xavier_uniform_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "PyTorch provides two data primitives: ``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``\n",
    "that allow you to use pre-loaded datasets as well as your own data.\n",
    "``Dataset`` stores the samples and their corresponding labels, and ``DataLoader`` wraps an iterable around\n",
    "the ``Dataset`` to enable easy access to the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset definition\n",
    "# A custom Dataset class must implement three functions: __init__, __len__, and __getitem__.\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    # The __init__ function is run once when instantiating the Dataset object\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        df = read_csv(path, header=None)\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, :-1]\n",
    "        original_labels = df.values[:, -1]\n",
    "        # ensure input data is floats\n",
    "        self.X = self.X.astype('float32')\n",
    "        # label encode target and ensure the values are floats\n",
    "        # da m7tago lo al labels bta3ti strings w ana 3ayzha numbers 34an adeha lel computer\n",
    "        self.y = LabelEncoder().fit_transform(original_labels)\n",
    "        \n",
    "        # this dictionary \n",
    "        self.encoding_mapping = {encoded_label: original_label for encoded_label, original_label  in zip(self.y, original_labels)}\n",
    "\n",
    "        # ensure the target is float\n",
    "        self.y = self.y.astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    # The __len__ function returns the number of samples in our dataset.\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    # The __getitem__ function loads and returns a sample from the dataset at the given index idx\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    # al function de bt2sm al data le train we test\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "\n",
    "# prepare the dataset\n",
    "def prepare_data(path):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    # The Dataset retrieves our dataset’s features and labels one sample at a time.\n",
    "    # While training a model, we typically want to pass samples in “minibatches”,\n",
    "    # reshuffle the data at every epoch to reduce model overfitting,\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return dataset.encoding_mapping, train_dl, test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 10) #ali 5arg mn al layer de hykon al size bta3o 10\n",
    "        # initializes the weights of the self.hidden1 layer using the Kaiming (He) Uniform initialization method\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(10, 8) #in= 10 34an ali abli 10, out = 8 \n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(8, 1)\n",
    "        # initializes the weights of the self.hidden3 layer using the Xavier (Glorot) Uniform initialization method.\n",
    "        #it is commonly used for layers with hyperbolic tangent (tanh) or sigmoid activation functions.\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.act3 = Sigmoid()\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = BCELoss() # Binary Cross-Entropy\n",
    "    # Stochastic Gradient Descent Optimizer\n",
    "    # model.parameters(): model weights\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(100):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients stored in the memory as the defualt behavior of pytorch is to sum the gradients\n",
    "            # but we want to use gradients of each iteration separatly so we initialize the gradients of zeros\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        # detach(): bykon fe graph fe pytorch byrbot ben al tensors fa detach btfsl al tensor da 3n ali ablo\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round to class values\n",
    "        yhat = yhat.round()\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Predictioin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model: Module):\n",
    "    # convert row to data\n",
    "    model.eval() # This is equivalent with self.train(False)\n",
    "    # model.eval : bt2ol lel model en de predict phase fa ma3tml4 operations mo3yna 34an twfr computations like drop out layers in NN m4 bst5dmha fel testing\n",
    "    # bt2fl kol al layers ali m4 m7tagha fel tetsing also storing the gradients in cache is not needed so we avoid wasting memory\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training (main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001DD55FD04C0>\n",
      "235 116\n",
      "Accuracy: 0.905\n",
      "Predicted: 1.000 (class=1) which is (class=g)\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "# dataset link https://datahub.io/machine-learning/ionosphere\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
    "encoding_mapping, train_dl, test_dl = prepare_data(path)\n",
    "print(train_dl)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "# define the network\n",
    "model = MLP(34)\n",
    "\n",
    "model.train(True) # set it to False for inference \n",
    "\n",
    "\n",
    "# train the model\n",
    "train_model(train_dl, model)\n",
    "# evaluate the model\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "# make a single prediction (expect class=1)\n",
    "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "yhat = predict(row, model)\n",
    "\n",
    "print('Predicted: %.3f (class=%d) which is (class=%s)' % (yhat, yhat.round(), encoding_mapping[int(yhat.round())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
